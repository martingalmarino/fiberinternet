# ğŸ¤– Sistema de AutomatizaciÃ³n - Telecom & Home Digital DK

## ğŸ“… ProgramaciÃ³n de Actualizaciones

### ğŸ—“ï¸ Cronograma Implementado

| Tipo | Frecuencia | Horario | DescripciÃ³n |
|------|------------|---------|-------------|
| **Full Update** | Semanal | Lunes 4:00 AM UTC (6:00 AM Copenhague) | ActualizaciÃ³n completa de todos los proveedores |
| **Light Check** | 2x por semana | MiÃ©rcoles y Viernes 2:00 PM UTC (4:00 PM Copenhague) | VerificaciÃ³n rÃ¡pida de cambios de precios |
| **Manual QA** | Mensual | Manual | RevisiÃ³n manual de datos clave |

### âš™ï¸ Workflows de GitHub Actions

#### 1. **Weekly Full Update** (`scraper-schedule.yml`)
- **Trigger**: AutomÃ¡tico (cron) + Manual
- **Funciones**:
  - Scraping completo de todos los proveedores
  - DetecciÃ³n de cambios en datos
  - Commit automÃ¡tico solo si hay cambios
  - Deploy automÃ¡tico a Vercel
  - Notificaciones de estado

#### 2. **Light Check** (`scraper-light-check.yml`)
- **Trigger**: AutomÃ¡tico (cron) + Manual
- **Funciones**:
  - VerificaciÃ³n rÃ¡pida de proveedores clave
  - DetecciÃ³n de cambios de promociones
  - Reporte de estado sin commit automÃ¡tico

### ğŸ›ï¸ Modos de Scraping

El scraper soporta 3 modos diferentes:

```bash
# Modo completo (por defecto)
SCRAPER_TYPE=full python scrape_fiber.py

# VerificaciÃ³n ligera
SCRAPER_TYPE=light python scrape_fiber.py

# Modo de prueba
SCRAPER_TYPE=test python scrape_fiber.py
```

### ğŸš€ EjecuciÃ³n Manual

#### Desde GitHub Actions Dashboard:
1. Ve a **Actions** en el repositorio
2. Selecciona **"Fiber Internet Scraper - Weekly Update"**
3. Click **"Run workflow"**
4. Configura los parÃ¡metros:
   - **Scraper Type**: `full`, `light`, o `test`
   - **Force Update**: `true` para forzar actualizaciÃ³n

#### Desde Terminal (desarrollo local):
```bash
cd scraper

# Scraping completo
python scrape_fiber.py

# VerificaciÃ³n ligera
SCRAPER_TYPE=light python scrape_fiber.py

# Modo de prueba
SCRAPER_TYPE=test python scrape_fiber.py
```

### ğŸ“Š Monitoreo y Logs

#### Logs de GitHub Actions:
- **UbicaciÃ³n**: Repositorio â†’ Actions â†’ [Workflow Name]
- **Contenido**: 
  - Estado de scraping por proveedor
  - NÃºmero de planes encontrados
  - Errores y warnings
  - Resumen de cambios

#### Logs Locales:
- **Archivo**: `scraper/scraper.log`
- **RotaciÃ³n**: Manual (recomendado limpiar semanalmente)

### ğŸ”§ ConfiguraciÃ³n de Proveedores

#### Habilitar/Deshabilitar Proveedores:
Edita `scraper/scraper.py`:

```python
PROVIDERS = [
    {
        'name': 'Provider Name',
        'scraper': scrape_provider,
        'enabled': True  # Cambiar a False para deshabilitar
    }
]
```

#### AÃ±adir Nuevos Proveedores:
1. Crear archivo en `scraper/providers/nuevo_provider.py`
2. Implementar funciÃ³n `scrape_nuevo_provider()`
3. AÃ±adir a la lista `PROVIDERS` en `scraper.py`

### ğŸ“ˆ MÃ©tricas y KPIs

#### MÃ©tricas AutomÃ¡ticas:
- **Total de proveedores activos**
- **NÃºmero de planes por proveedor**
- **Tasa de Ã©xito de scraping**
- **Frecuencia de cambios detectados**

#### Dashboard de Estado:
- **Frontend**: Muestra "Data opdateret: [fecha]"
- **GitHub**: Summary automÃ¡tico en cada workflow
- **Logs**: Historial completo de ejecuciones

### ğŸš¨ Manejo de Errores

#### Errores Comunes:
1. **Proveedor no disponible**: Se omite y continÃºa con otros
2. **Cambios en estructura web**: Requiere actualizaciÃ³n del scraper
3. **Rate limiting**: Implementar delays entre requests
4. **Certificados SSL**: Manejo automÃ¡tico de warnings

#### Alertas:
- **GitHub Actions**: Notificaciones automÃ¡ticas en fallos
- **Logs**: Warnings para proveedores con problemas
- **Frontend**: Mensaje de error si no se pueden cargar datos

### ğŸ”„ EvoluciÃ³n del Sistema

#### Fase Actual (MVP):
- âœ… 1 actualizaciÃ³n semanal completa
- âœ… 2 verificaciones ligeras por semana
- âœ… 15 proveedores activos
- âœ… Deploy automÃ¡tico a Vercel

#### Fase de Crecimiento:
- ğŸ“ˆ 2-3 actualizaciones semanales
- ğŸ“ˆ Monitoreo en tiempo real de promociones
- ğŸ“ˆ Alertas automÃ¡ticas para cambios significativos
- ğŸ“ˆ API endpoints para integraciones

#### Fase Avanzada:
- ğŸš€ Actualizaciones en tiempo real
- ğŸš€ Machine learning para detecciÃ³n de patrones
- ğŸš€ IntegraciÃ³n con sistemas de afiliados
- ğŸš€ Dashboard administrativo

### ğŸ› ï¸ Mantenimiento

#### Tareas Semanales:
- [ ] Revisar logs de scraping
- [ ] Verificar que todos los proveedores respondan
- [ ] Actualizar selectores CSS si es necesario

#### Tareas Mensuales:
- [ ] RevisiÃ³n manual de precios y promociones
- [ ] AnÃ¡lisis de proveedores con baja tasa de Ã©xito
- [ ] OptimizaciÃ³n de performance del scraper
- [ ] ActualizaciÃ³n de dependencias

#### Tareas Trimestrales:
- [ ] EvaluaciÃ³n de nuevos proveedores
- [ ] RevisiÃ³n de estrategia de scraping
- [ ] AnÃ¡lisis de competencia
- [ ] OptimizaciÃ³n de costos de infraestructura

---

## ğŸ“ Soporte

Para problemas o preguntas sobre el sistema de automatizaciÃ³n:
1. Revisar logs en GitHub Actions
2. Verificar estado de proveedores individuales
3. Consultar documentaciÃ³n del scraper en `scraper/README.md`
4. Contactar al equipo de desarrollo

**Ãšltima actualizaciÃ³n**: Septiembre 2024
